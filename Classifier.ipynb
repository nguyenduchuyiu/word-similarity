{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  \n",
    "            vector = np.array(list(map(float, values[1:])))  \n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "embeddings_path = 'dataset/W2V_150.txt'\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "def get_word_vector(word):\n",
    "    if word in embeddings:\n",
    "        return embeddings[word]\n",
    "    else:\n",
    "        return np.zeros(150)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2578754 , -0.9726154 , -1.724687  ,  0.1039191 ,  0.7886895 ,\n",
       "       -0.9466447 ,  0.4693937 ,  1.152593  ,  0.1155706 ,  0.1692931 ,\n",
       "        0.7421699 , -0.787329  , -0.6093806 ,  1.025403  ,  0.6054297 ,\n",
       "       -1.236188  , -0.9221155 , -1.210821  , -0.2781016 ,  1.110141  ,\n",
       "        1.728644  , -0.8907079 , -0.7430073 , -1.386758  , -0.6328576 ,\n",
       "        0.4824337 , -0.4921961 , -0.5655223 , -1.927649  , -0.4070618 ,\n",
       "        0.181633  ,  0.7142181 , -0.8746694 ,  0.8621774 ,  0.3763472 ,\n",
       "       -0.2323172 , -1.696975  , -0.6795936 , -0.7101342 , -0.3980412 ,\n",
       "        0.8449571 , -0.9750128 ,  1.558469  , -1.674035  , -0.680132  ,\n",
       "       -0.97935   ,  0.07189509, -0.2478063 , -0.4585635 , -0.6954154 ,\n",
       "        2.041032  ,  2.368579  ,  0.9618701 ,  1.230371  ,  2.721321  ,\n",
       "        0.5995511 ,  0.8380752 ,  0.8573769 ,  1.95823   ,  1.082221  ,\n",
       "        0.6791626 , -0.9185423 ,  0.3077531 ,  0.2545303 , -1.09807   ,\n",
       "        0.07830691, -0.7900249 , -0.3596387 , -0.5374528 ,  0.7045696 ,\n",
       "       -1.405178  ,  0.748323  , -0.9748238 ,  0.505672  , -1.064186  ,\n",
       "       -0.09893318, -0.3444446 , -1.785952  ,  2.029789  , -0.6841028 ,\n",
       "       -3.018026  , -0.0218592 , -0.03212921, -0.8394837 ,  0.1017763 ,\n",
       "        1.469285  ,  0.517174  , -0.8083785 , -0.7863334 , -0.02706276,\n",
       "        1.647663  , -0.4082772 , -1.531167  , -1.613358  , -1.267038  ,\n",
       "        0.4993207 , -0.5925277 , -0.3438716 , -1.337588  , -0.03321799,\n",
       "       -0.5446152 ,  0.6069489 , -1.289287  ,  0.4370298 ,  2.350519  ,\n",
       "       -0.08258818, -0.5376267 , -1.415858  ,  1.383907  , -1.662229  ,\n",
       "        0.2631687 ,  1.319684  , -0.8904743 , -1.655962  , -2.524005  ,\n",
       "       -0.7317258 , -3.200265  ,  0.2673278 , -1.427984  ,  1.147003  ,\n",
       "        0.6494801 , -1.415339  ,  0.2135608 ,  1.755648  ,  0.4420949 ,\n",
       "       -0.4785798 , -1.011857  ,  0.4852979 , -1.321548  ,  2.474401  ,\n",
       "        0.657818  ,  0.5786278 ,  1.582759  , -1.149667  , -2.565566  ,\n",
       "       -2.116292  ,  0.8363985 ,  0.4458983 ,  0.4037514 , -1.38201   ,\n",
       "       -0.2172358 , -1.529086  , -0.3343721 ,  0.3834576 ,  0.697854  ,\n",
       "        0.4732894 , -0.6912606 ,  1.344066  ,  0.05758414,  0.7924209 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector('chó')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line, not enough values: ['bằng_chứng']\n",
      "Skipping line, not enough values: ['dài_ngoằng']\n",
      "Skipping line, not enough values: ['dầu_nhớt']\n",
      "Skipping line, not enough values: ['diễn_dịch']\n",
      "Processed pairs: 13556, Labels: 13556\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "with open('./antonym-synonym/Antonym_vietnamese.txt', 'r', encoding='utf-8') as f:\n",
    "    next(f)  \n",
    "    for line in f:\n",
    "        if line.strip() == \"\":  \n",
    "            continue\n",
    "        values = line.strip().split(' ')\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            print(f\"Skipping line, not enough values: {values}\")  \n",
    "            continue\n",
    "        \n",
    "        word1 = values[0]\n",
    "        word2 = values[1]\n",
    "        \n",
    "        vector1 = get_word_vector(word1)\n",
    "        vector2 = get_word_vector(word2)\n",
    "        \n",
    "        vector = np.concatenate([vector1, vector2])\n",
    "        pairs.append(vector)\n",
    "        labels.append(0)  \n",
    "\n",
    "\n",
    "with open('./antonym-synonym/Synonym_vietnamese.txt', 'r', encoding='utf-8') as f:\n",
    "    next(f)  \n",
    "    for line in f:\n",
    "        if line.strip() == \"\":  \n",
    "            continue\n",
    "        values = line.strip().split(' ')\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            print(f\"Skipping line, not enough values: {values}\") \n",
    "            continue\n",
    "        \n",
    "        word1 = values[0]\n",
    "        word2 = values[1]\n",
    "        \n",
    "        vector1 = get_word_vector(word1)\n",
    "        vector2 = get_word_vector(word2)\n",
    "        \n",
    "        vector = np.concatenate([vector1, vector2])\n",
    "        pairs.append(vector)\n",
    "        labels.append(1)  \n",
    "\n",
    "print(f\"Processed pairs: {len(pairs)}, Labels: {len(labels)}\")\n",
    "\n",
    "X_train, y_train = np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            \n",
    "            values = line.split()  \n",
    "            if len(values) < 3:  \n",
    "                print(f\"Skipping line, not enough values: {values}\")  \n",
    "                continue\n",
    "            \n",
    "            word1 = values[0]\n",
    "            word2 = values[1]\n",
    "            label = values[2]  \n",
    "            \n",
    "\n",
    "            vector1 = get_word_vector(word1)\n",
    "            vector2 = get_word_vector(word2)\n",
    "            \n",
    "\n",
    "            vector = np.concatenate([vector1, vector2])\n",
    "            pairs.append(vector)\n",
    "            labels.append(0 if label == 'ANT' else 1)  \n",
    "\n",
    "    return np.array(pairs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logistic = LogisticRegression(max_iter=1000)\n",
    "clf_logistic.fit(X_train, y_train)\n",
    "\n",
    "clf_mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, activation='relu')\n",
    "clf_mlp.fit(X_train, y_train)\n",
    "\n",
    "def evaluate(y_true, y_pred, model_name):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{model_name} - Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Precision: 0.5706051873198847, Recall: 0.9850746268656716, F1-Score: 0.7226277372262774\n",
      "MLP - Precision: 0.9757281553398058, Recall: 1.0, F1-Score: 0.9877149877149877\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_data('./dataset/ViCon-400/400_noun_pairs.txt')\n",
    "len(X_test), len(y_test)\n",
    "y_pred_logistic = clf_logistic.predict(X_test)\n",
    "y_pred_mlp = clf_mlp.predict(X_test)\n",
    "# Logistic Regression Evaluation\n",
    "evaluate(y_test, y_pred_logistic, 'Logistic Regression')\n",
    "\n",
    "# MLP Evaluation\n",
    "evaluate(y_test, y_pred_mlp, 'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_verb_test, y_verb_test = load_data('./dataset/ViCon-400/400_verb_pairs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - Precision: 0.9852941176470589, Recall: 1.0, F1-Score: 0.9925925925925926\n",
      "Logistic Regression - Precision: 0.599388379204893, Recall: 0.9751243781094527, F1-Score: 0.7424242424242424\n"
     ]
    }
   ],
   "source": [
    "ypred_mlpverb = clf_mlp.predict(X_verb_test)\n",
    "ypred_logisticverb = clf_logistic.predict(X_verb_test)\n",
    "evaluate(y_verb_test, ypred_mlpverb, 'MLP')\n",
    "evaluate(y_verb_test, ypred_logisticverb, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adj_test, y_adj_test = load_data('./dataset/ViCon-400/600_adj_pairs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Precision: 0.6953125, Recall: 0.8870431893687708, F1-Score: 0.7795620437956204\n",
      "MLP - Precision: 1.0, Recall: 1.0, F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "ypred_mlpadj = clf_mlp.predict(X_adj_test)\n",
    "ypred_logisticadj = clf_logistic.predict(X_adj_test)\n",
    "evaluate(y_adj_test, ypred_logisticadj, 'Logistic Regression')\n",
    "evaluate(y_adj_test, ypred_mlpadj, 'MLP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
